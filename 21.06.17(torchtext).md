자연어처리는 pytorch로 많이 하는 것 같아서 좀 배워봐야지. 오디오는 확실히 Pytorch로 많이 하고, 이미지는 keras로 많이 하는듯

https://github.com/yunjey/pytorch-tutorial

https://wikidocs.net/65794

```python
#1. 데이터 불러오기
import urllib.request
import pandas as pd
urllib.request.urlretrieve("https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv", filename="IMDb_Reviews.csv")
df = pd.read_csv('IMDb_Reviews.csv', encoding='latin1')
df.head()
train_df = df[:25000]
test_df = df[25000:]
train_df.to_csv("train_data.csv", index=False) #csv로 굳이 다시 저장해아하나?
test_df.to_csv("test_data.csv", index=False)

# 2. 필드 정의하기 -> 이부분이 특이함. 전처리할 계획을 짜는 단계
from torchtext import legacy # torchtext.data 임포트
#필드를 통해 어떤 전처리를 할지 정함. 아직 전처리한 것 아님!!!
# 필드 정의
TEXT = legacy.data.Field(sequential=True, #시퀀스 데이터 여부
                  use_vocab=True, #단어 집합을 만들 것인지
                  tokenize=str.split, #어떤 토큰화함수를 사용할 것인지
                  lower=True, #소문자화
                  batch_first=True, #미니 배치 차원을 맨 앞으로 하여 데이터를 불러올 것인지 여부(default는 false)
                  fix_length=20) #최대 허용 길이. 여기 맞춰 패딩작업

LABEL = legacy.data.Field(sequential=False,
                   use_vocab=False,
                   batch_first=False,
                   is_target=True) #레이블 데이터 여부(default는 false)
                   
#3. 전처리
from torchtext.legacy.data import TabularDataset #얘도 레거시화되었네
train_data, test_data = TabularDataset.splits(path='.', train='train_data.csv',test='test_data.csv', format='csv',
                                              fields=[('text',TEXT),('label',LABEL)], skip_header=True)
#path: 파일 위치 경로, format: 데이터 포맷, fields: 위에서 정의한 필드를 지정. 첫번째 원소는 데이터셋 내에서 해당 필드를 호칭할 이름
#skip_header: 데이터의 첫번째 줄은 무시

#4. vocab 만들기
TEXT.build_vocab(train_data, min_freq=10, max_size=10000)

#5. 데이터로더 만들기
from torchtext.legacy.data import Iterator
batch_size = 5
train_loader = Iterator(dataset=train_data, batch_size=batch_size)
test_loader = Iterator(dataset=test_data, batch_size=batch_size)
#첫번째 미니배치
batch=next(iter(train_loader))
```

https://towardsdatascience.com/nlp-classification-with-universal-language-model-fine-tuning-ulmfit-4e1d5077372b

ctc loss beam search https://distill.pub/2017/ctc/
