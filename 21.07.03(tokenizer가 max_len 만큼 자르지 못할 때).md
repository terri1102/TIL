일단, 왜 이런 에러가 발생하는지 모르겠다. 분명히 tokenizer에 max_len=512로 주고 truncate=True로 했는데, 왜 그 길이 이상으로 나오는지...

그래서 일단 tokenizer에 넣기 전에 sentence1과 sentence2의 길이가 각각 256이 넘으면 256까지만 사용하게 하고, tokenizer에 넣었더니 모델이 돌아가기는 한다.
그런데 모델 성능이 많이 떨어지지는 않을까 싶기도 하고...
짧은 문장으로 돌렸을 때는 이런 문제가 없었는데, 긴 문장으로 하니까 truncate가 왜 안 되는지 모르겠다.
